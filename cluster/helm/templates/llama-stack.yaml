apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-stack
spec:
  replicas: {{ .Values.llamaStack.replicaCount }}
  selector:
    matchLabels:
      app: llama-stack
  template:
    metadata:
      labels:
        app: llama-stack
    spec:
      containers:
      - name: stack
        image: {{ .Values.llamaStack.image }}
        volumeMounts:
        - name: config
          mountPath: /usr/local/lib/python3.12/site-packages/llama_stack/distributions/starter/
        ports:
        - containerPort: {{ .Values.llamaStack.port }} 
        resources: {{ toYaml .Values.llamaStack.resources | nindent 10 }}
        env:
            - name: LLAMA_STACK_PORT
              value: "8000"
      volumes:
      - name: config
        configMap:
          name: llama-stack-config 
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
data:
  run.yaml: |
    version: 2
    image_name: starter
    apis:
    - inference
    providers:
      inference:
      - provider_id: ollama
        provider_type: remote::ollama
        config:
          url: http://ollama.default.svc.cluster.local:11434
    storage:
      backends:
        kv_default:
          type: kv_sqlite
          db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/kvstore.db
        sql_default:
          type: sql_sqlite
          db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/sql_store.db
      stores:
        metadata:
          namespace: registry
          backend: kv_default
        inference:
          table_name: inference_store
          backend: sql_default
          max_write_queue_size: 10000
          num_writers: 4
        conversations:
          table_name: openai_conversations
          backend: sql_default
    registered_resources:
      models:
      - provider_id: ollama
        model_id: "ollama/gemma3:latest"
        provider_model_id: "gemma3:latest"
      shields: []
      vector_dbs: []
      datasets: []
      scoring_fns: []
      benchmarks: []
      tool_groups:
      - toolgroup_id: builtin::websearch
        provider_id: tavily-search
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
    server:
      port: 8321
    telemetry:
      enabled: true
    vector_stores: {}
---
apiVersion: v1
kind: Service
metadata:
    name: llama-stack
spec:
    selector: 
        app: llama-stack
    ports:
    - port: {{ .Values.llamaStack.port }}
      targetPort: {{ .Values.llamaStack.port }}
    type: LoadBalancer
